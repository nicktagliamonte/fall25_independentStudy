Title: vn-IPFS (paper_restyled) — Plain-English Summary and Symbol Guide

Purpose
This note explains the core ideas, results, and evidence from paper_restyled.tex in plain, non-technical language. It also includes a glossary for the math symbols used, so you can justify the arguments to non-math audiences.

Big Picture (What this system does)
- Goal: reliable file replication in decentralized networks without DHTs (global routing tables) or central coordinators.
- Coordination (who should store a copy) happens via a shared "tuple space" (think: a shared bulletin board of small facts) spread over a lightweight ring overlay. Actual bytes move over libp2p streams.
- A simple placement rule (N-hop exclusion) pushes replicas away from the uploader’s neighborhood so copies end up meaningfully far apart.
- The ring layer (UVR) keeps only neighbor pointers (predecessor/successor) and fixes broken links locally when nodes fail. No global tables.

Key Ideas by Section
1) Introduction
- Problem: Replication clumps near the uploader or depends on global tables that are fragile under churn/partitions.
- Solution: Separate coordination from transport. Use a ring + tuple space for intent/state and libp2p streams for bytes. Enforce N-hop exclusion for outward diffusion.

2) Related Work (short context)
- DHTs optimize lookups but impose global invariants and repair costs. Gossip and CRDT-style (merge-without-conflict) techniques scale with local rules. This work favors local rules and convergence-friendly state over global routing.

3) Motivation vs. DHTs
- Why not DHTs: global tables to maintain; repair under churn; no direct control of replica placement.
- Our benefits: local invariants, bounded local repairs, explicit outward placement.

4) UVR Tuple-Space Semantics (how it is organized)
- UVR provides neighbor maintenance, best-effort spreading of facts, and local failure repair.
- Tuple space holds intent, progress, and compact summaries; idempotent merges prevent corruption from duplicates.

5) Replication Theory: N-Hop Exclusion (why copies go outward)
- Rule: when a node accepts a replica, it excludes itself and its neighbors from the next placements. Later acceptances therefore tend to be farther away from the origin.
- This creates an outward-moving frontier in expectation without needing full network paths.

6) Formal Proofs (what is guaranteed, explained simply)
6a) Convergence after things calm down (finite-height convergence)
- Claim: After a calm time T (no new facts), and with fair repeated delivery, every node’s state stabilizes to the same final state J.
- Premises: (P1) No new facts after T. (P2) Every update only adds information (idempotent, monotone merge). (P3) The space of facts is finite (finite-height). (P4) Within a connected component, any fact that is re-emitted indefinitely is delivered to everyone infinitely often (fairness).
- Goal: Show that for each node v, after some time T' ≥ T, x_v(t) = J for all t ≥ T'.
- Steps:
  1) Upper bound: Because new facts stop after T and merges only add, each x_v(t) can never exceed J (the union/merge of all facts present at T). So x_v(t) ⊑ J for all t ≥ T.
  2) Finite increases: Since facts come from a finite set and merges are idempotent, each node can only add a new fact finitely many times before it has seen everything in J.
  3) Eventual inclusion: Pick any single fact f that is present somewhere at time T. Because of connectedness and fairness, f (or messages carrying it) is seen infinitely often at every node. If v is missing f, the next time f arrives v merges it; by idempotence, duplicates change nothing.
  4) Exhaustion: Repeat Step 3 over all facts in J. There are finitely many, so after finitely many successful merges, every node contains all of J.
  5) Stabilization: Once x_v(t) = J, later merges are no-ops by idempotence. Hence each node stabilizes and all stabilized states are equal to J.
- Why valid: Each step uses only monotone growth, finiteness, and fairness. No retractions are needed, so order and delays don’t hurt correctness.

6b) Replication liveness (it finishes)
- Claim: Within a connected component with enough distinct eligible nodes, the process completes all n placements after T.
- Premises: (P1) After T the component is strongly connected with fair delivery. (P2) Re-emissions continue (no hard TTL before completion). (P3) Eligibility is greedy and idempotent: an eligible node accepts once, then becomes ineligible. (P4) Capacity: at least n distinct eligible nodes exist and remain live.
- Goal: Show that there is a finite time T' by which n distinct nodes have accepted and the remaining count reaches zero.
- Steps:
  1) Progress opportunity: If remaining n(t) > 0 and some node is eligible, fairness ensures that node will observe the tuple infinitely often.
  2) Greedy acceptance: On observing the tuple with n > 0, the eligible node accepts (policy), reducing n by exactly one.
  3) Idempotence: That node cannot accept again for the same content (already stored; now excluded), so each acceptance is unique and permanent.
  4) Bounded steps: At most the initial n acceptances are needed; after each acceptance, n decreases by one and never increases.
  5) Finite time: Re-emissions occur within bounded intervals, deliveries occur under fairness, so each acceptance occurs after a finite waiting time. Summing finitely many such waits yields finite completion time.
- Why valid: Monotone decrease of n and one-shot acceptance prevent oscillation; fairness prevents starvation; capacity ensures we don’t run out of distinct acceptors.

6c) Local repair time (bounded)
- Claim: When a successor fails, ring invariants are restored within a bounded time: tau_repair ≤ tau_fd + H + F.
- Premises: (P1) Heartbeats to the successor occur at most every H_hb units. (P2) Suspicion after m consecutive misses: tau_fd ≤ m*H_hb + F. (P3) Once suspicious, control messages are emitted within H and delivered within F (fairness window). (P4) Local successor recomputation preserves ring invariants and completes once messages are delivered.
- Goal: Bound time from crash to restored ring.
- Steps:
  1) Detection bound: After the crash, missed heartbeats trigger suspicion within tau_fd ≤ m*H_hb + F.
  2) Repair initiation: Upon suspicion, the predecessor emits repair/control tuples within H.
  3) Delivery: Those tuples reach relevant neighbors within an additional F.
  4) First consistent update: With those messages, the predecessor installs a new live successor, restoring invariants locally.
  5) Total bound: Summing yields tau_repair ≤ tau_fd + H + F.
- Why valid: Each phase is guarded by a local timer or fairness bound; idempotent updates ensure extra messages do not break invariants.

6d) Safety and duplicate suppression (no oscillation)
- Claim: Accepting the same content twice at a node has no effect, and the remaining count n never increases.
- Premises: (P1) Store is idempotent (writing an already stored content is a no-op). (P2) Acceptance adds the node to the exclusion set permanently. (P3) Only acceptances change n and each acceptance decrements n by one.
- Steps:
  1) At-most-once: After acceptance, the node stores the content and is excluded; subsequent deliveries don’t re-enable acceptance.
  2) Monotone n: Only acceptances affect n; duplicates are no-ops; thus n(t) is nonincreasing.
  3) Clean termination: When n reaches 0, eligibility is effectively off everywhere and the process halts.
- Why valid: Idempotence and monotone exclusion prevent double counting and oscillation.

6e) N-hop frontier growth (why placements move outward)
- Claim: The excluded region E_k grows monotonically and, when a new acceptance covers the next shell, the covered radius increases by one.
- Premises: (P1) After k acceptances A_k = {a_1, …, a_k}, E_k = B(o, h0) ∪ (union over i of B(a_i, h)). (P2) Acceptance at step k+1 occurs only outside E_k. (P3) The “covered radius” R_k is the largest r with B(o, r) ⊆ E_k.
- Goal: Show: (i) E_{k+1} ⊇ E_k and R_{k+1} ≥ R_k; (ii) if B(a_{k+1}, h) intersects the next shell S(o, R_k+1), then R_{k+1} ≥ R_k+1.
- Steps:
  1) Monotone frontier: E_{k+1} = E_k ∪ B(a_{k+1}, h) ⊇ E_k, so anything covered before remains covered. Hence R_{k+1} ≥ R_k.
  2) No re-entry: Since acceptance requires a_{k+1} ∉ E_k, it cannot be “inside” previously covered neighborhoods.
  3) Hitting the next shell: If B(a_{k+1}, h) overlaps S(o, R_k+1), then E_{k+1} contains all of B(o, R_k) plus some of the next shell; thus B(o, R_k+1) ⊆ E_{k+1}.
  4) Radius increase: Therefore R_{k+1} ≥ R_k+1.
- Why valid: Exclusions are unions of neighborhoods; unions only grow with new acceptances. The shell argument formalizes “outward drift” without needing shortest paths.

7) Experiments (evidence it works)
- Feasibility: Ring formation time grows roughly linearly with the number of nodes and has low variability. End-to-end replication shows steady latency with higher throughput for larger files.
- Adaptability: Microbenchmarks are consistent across hop counts with peaks at 16 and 512; throughput scales with offered load until resource limits.
- Robustness: With induced failures, neighbor-only repair recovers 100% with ~100 ms mean repair. Tuple spreading shows fast completion for most messages.

8) Significance and Tradeoffs
- Significance: Achieves outward diffusion without global routing tables, keeps repairs local, and relies on monotone (never backtracking) coordination.
- Tradeoffs: More coordination messages than direct point-to-point control, but much simpler failure handling and no global table maintenance.

9) Practical Implications (short operational view)
- Use tuple space + UVR for coordination and libp2p for bytes. Enforce N-hop exclusion at the app layer. Prefer explicit bootstraps over mDNS. Publish compact ring-info and replication-status. Sign and rate-limit control traffic.

Assumptions and Limits (in plain terms)
- Honest-but-fallible nodes: crashes and message drops can happen, but no lying (non-Byzantine).
- Eventual connectivity and fairness: after a calm period, repeated messages reach everyone in a connected component.
- No new facts during the calm period for the convergence guarantee.
- In partitions, each component progresses independently; when reconnected, time limits and versions prevent replay storms.

Glossary: Symbols and Terms (plain-English, ASCII names)
- G = (V, E): network graph. V = nodes (computers). E = links (who can talk to whom).
- R = (V, A): ring overlay. A = directed successor links (each node points to a successor).
- T = (cid, n, excluded, originator): replication task (content ID, remaining replicas needed, who is excluded, who started it).
- B(v, r): nodes within r hops from v (everyone up to r steps away).
- S(v, r): nodes exactly r hops from v (the ring at distance r).
- E_k: excluded set after k acceptances (places we will not place next).
- R_k: largest radius around the originator fully covered by exclusions after k acceptances (how far the no-go zone has grown).
- n: remaining replica count (counts down to 0).
- J = sqcup_u x_u(T): merged final state across a component at time T. "sqcup" is a join/merge that adds missing facts without duplicates.
- sqcup (join): merge two states by adding whatever one has that the other lacks, with deduplication.
- sqsubseteq (is contained in): X sqsubseteq Y means Y includes all info in X (and possibly more).
- cup (union): combine two sets into one containing all elements from both.
- subseteq (subset): X subseteq Y means every element of X is in Y.
- |X| (cardinality): the number of elements in set X.
- O(1): constant size or cost; does not grow with network size.
- tau_fd: failure detection time bound.
- tau_repair: repair completion time bound.
- H_hb: heartbeat interval to check successors.
- m: miss threshold for suspicion (consecutive missed heartbeats).
- F: fairness window (max delay to deliver messages that are retried indefinitely).
- H: retransmission bound (max interval between re-emissions).
- N (naturals): 0, 1, 2, ...
- TTL: time-to-live; a limit after which a message is discarded.
- Idempotent: doing the same update twice has the same effect as once.
- Monotone: state only grows by adding new facts; nothing is removed.
- Quiescent epoch: a calm period when no new facts are injected.
- Fairness: if a message is retried forever, everyone eventually sees it.
- CRDT: a data type that merges in any order without conflicts.
- UVR: Unidirectional Virtual Ring; each node keeps predecessor/successor, enabling simple repairs and predictable spreading.
- Tuple space: shared board of small facts; nodes put/get facts and the system spreads them.

How to explain the proofs to non-mathematicians
- Convergence: After things stop changing and people keep sharing, everyone ends up with the same full set of facts, and then nothing changes anymore.
- Liveness: If you need N placements and each placement reduces the remaining count by one, and nobody places twice, then after N placements you are done.
- Repair time: Because we check neighbors regularly and have a fallback, there is a bound on how long it takes to fix a broken link.

Takeaways
- Simple local rules (neighbor-only ring, outward placement, idempotent merges) can replace global routing tables for replication placement.
- Separating coordination (facts) from transport (bytes) makes failures easier to tolerate and reason about.
- The math formalizes common-sense guarantees: eventual agreement on facts, guaranteed completion given enough candidates, and bounded repair time.
