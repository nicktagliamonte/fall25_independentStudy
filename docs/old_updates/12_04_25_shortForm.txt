P2P changes to support SNG N‑hop and node repair
- Core
  - Embed API: `pkg/node` exposes `Options`, `Service`, `Start(ctx, opts)`.
  - Helpers: `Status`, `PutRaw`, `GetRawFrom`, `ListImmediatePeerIDs`, `RestoreFromManifest`.
  - Lightweight manifest index under `/manifest/index/<cid>` for snapshots.
- Control server
  - Added: `/shutdown`, `/restore` (POST), `/restore/status`, `/snapshot`, `/neighbors`, `/id`.
  - Kept: `/health`, `/metrics`, `/put`, `/get`, `/connect`, `/events`.
- Metrics
  - New counters: `restores_started`, `restores_ok`, `restores_failed`, `restore_bytes` (in `/metrics`).
- Identity
  - Stable PeerID via `--key` or `Options.KeyPath`; `/id` endpoint; `node keygen` helper.
- Tests
  - Unit test for `RestoreFromManifest` (concurrency, timeouts, partial failures). All tests pass.

How do you repair damaged or unresponsive nodes?
- At the tuple space level, we decide when a node is bad, restart it, and asks this p2p layer to refill its content.
  - SNG detects inconsistency (e.g., majority‑hash mismatch) and quarantines the node (call `/shutdown` or `Service.Close`).
  - After restart with the same key (stable PeerID), SNG sends a restore job:
    - Library: `Service.RestoreFromManifest(ctx, cids, concurrency, timeout, budget)`.
    - Daemon: `POST /restore` with the list of CIDs; poll `/restore/status` until done.
  - Indexing is automatic on put/get, so a fresh snapshot can be emitted via `/snapshot` and used for future repairs.
  - P2P itself is agnostic to “why” a node repairs; it only fetches blocks and maintains connections reliably.

Usage (operators)
- Start with a persistent identity
  - Generate key: `node keygen --out ~/.sng40/node.key`
  - Run daemon: `./node run --key ~/.sng40/node.key --daemon`
- Quarantine (graceful stop)
  - `node shutdown --control /tmp/fall25_node/daemon.json`
- Snapshot and restore
  - Snapshot: `node snapshot --control /tmp/fall25_node/daemon.json > snapshot.json`
  - Extract: `jq -r '.cids[]' snapshot.json > snapshot.txt`
  - Restore: `node restore --manifest snapshot.txt --concurrency 8 --timeout 30s --control /tmp/fall25_node/daemon.json`
- Inspect
  - `/metrics`, `/events`, `/neighbors`, `/id` via the control address in `/tmp/fall25_node/daemon.json`.

Boundary with SNG
- N‑hop orchestration, majority‑hash voting, and repair policy live in SNG.
- This repo provides the data‑plane and control hooks needed to execute those decisions.

Tuple/SNG notes
- SNG owns tuple schemas, N‑hop producer/consumer, exclusion TTL/backtracking, and majority‑hash policy. It emits replicate 
requests on PUT, claims them opportunistically, backtracks exclusions on expiry, and logs key transitions for observability. 
Transport wrapper, gateway clients, and cutover controls exist on the SNG side for phased rollout. Risks (churn, duplicates, 
long tails) are mitigated by idempotence, leases, TTL/backtracking, retries, and circuit breaking. Deliverables and tests 
remain tracked in SNG; this repo focuses on the p2p data‑plane and control hooks described above.