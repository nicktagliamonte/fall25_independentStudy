1. Security Plan (verifiable head/crypto safety) plan and motivation
  - the issue:
    - any software using libp2p can complete a base transport/security handshake if it knows our multiaddr
    - without extra policy, that means unknown code can momentarily connect and try to open streams
  - what i changed now:
    - verack runs immediately on connection; non-handshake streams are blocked until verification
    - if verack fails, we close the peer (the whole connection), not just the stream
  - what i changed (admission mode):
    - tokens: per-node token signed by a CA; carried in verack version; verified at connection time
    - optional by default: if no token envs set, node runs open (public), like IPFS at the app layer
    - private when configured: set SNG40_CA_PUBS and SNG40_TOKEN (or SNG40_REQUIRE_TOKEN=true) to require admission
    - state summary (still planned): advertise a verifiable head (head CID + height) for a simple G-set log; for progress/sync, not admission
    - after verack: optionally fetch suffix from head to converge state (time permitting)
  - why this approach:
    - keeps security/auth at the app boundary we control; reduces junk traffic; simple to operate (RELATIVELY)...
    - the verifiable head gives a monotone progress signal we can reason about and use to schedule sync
      - which i should maybe quit trying to shoehorn into this system.
  - what it’s not:
    - not a replacement for libp2p crypto; not a public‑internet firewall
    - we may still see a base connection from unknown software, but it’s dropped immediately on failed verack
  - reference:
    - implementation plan lives in docs/sec_todo.txt (credential flow, log format, sync, tests)
  - benefits of this decision:
    - mirrors IPFS’s open-by-default model while enabling ad hoc private nets via user-supplied CA keys
    - simple ops: no PSK distribution; just a CA pubkey and per-node tokens
    - lets third parties bring their own trust root to form independent private clusters

2. g-set inclusion and monotonicity
  - what is it:
    - a G‑set is a grow‑only set: you can add elements, you never remove them
    - in this system, the elements can be simple facts like “peer X passed verack here” (or later, events in an append‑only log)
  - what it does for me:
    - gives a clean, single‑direction notion of “progress”: once something is learned, it stays learned
    - i can summarize progress with a small number (|G|) or a head pointer (CID) instead of shipping the whole set
  - how we keep it monotone:
    - only ever perform set‑add; no deletes, no rewrites
    - persist the adds; on restart, reload them; the count/height never goes backwards
    - when two nodes sync, they union their facts (set union) — still only growth
    - if/when we use an append‑only log, each block links to the previous head; that structure enforces “no rollback”
  - why i want monotonicity:
    - monotone state is easy to reason about (CALM): convergence without coordination and fewer locky/fragile protocols
    - no rollbacks means simpler error handling and fewer weird edge cases under partial failure
    - it’s cheap to compare/schedule: higher height likely has more to teach me; i can fetch a suffix and be done
    - also plays nicely with background gossip: repeated unions are idempotent, so “try again later” is safe
  - where it happens in code:
    - `stack` creation wires up `Datastore` and `BlockSvc`, the substrate for the G‑set
    - handshake registration is done after `stack` exists so head/height are advertised
    - on success, adds are recorded via `AppendPeerAdded(ctx, stack.Datastore, stack.BlockSvc, <peer>)`
    - runs in: `run` (dialer + gossip timers), `connect` (explicit connect), `get` (inline fetch with static router)
    - ensuring `stack` exists before these paths makes G‑set additions consistent from process start

3. typed IPLD schemas for events
  - what changed:
    - replaced ad-hoc DAG-CBOR map building/parsing with typed IPLD (bindnode) for `peer_added` events
    - `AppendPeerAdded`, `SyncSuffix`, and `AppendPeerAddedIfNew` now use typed encode/decode helpers
  - terms and definitions:
    - IPLD: InterPlanetary Linked Data — a data model for content-addressed graphs; lets us define structured, self-describing data that many tools can read.
    - CBOR: Concise Binary Object Representation — a compact binary format similar to JSON; “DAG‑CBOR” is the deterministic CBOR profile IPLD uses so the same data encodes to the same bytes.
    - CID: Content Identifier — a hash-based ID of the exact bytes; identical bytes → identical CID.
  - why this is beneficial (plain terms):
    - fewer mistakes: the typed structure acts like a form that catches missing or wrong fields early.
    - easier to change safely: we can add new fields later without breaking old data or readers.
    - consistent IDs: deterministic encoding means the same content always produces the same CID.
    - better tooling and interoperability: standard IPLD tools can inspect and validate our data.
  - theory/support:
    - schema-first reduces accidental complexity (see “Out of the Tar Pit”, 2006)
    - data encoding and evolution best practices (Kleppmann, Designing Data-Intensive Applications, Ch. 4)
    - IPLD Schemas: typed data across content addressed systems (see `https://ipld.io/specs/schemas/`)
  - enforcement in code:
    - `internal/storage/typed.go`: `PeerAddedGo`, encode/decode helpers, `computeCBORCID`
    - `internal/storage/state.go`: write/read paths now use typed helpers instead of manual maps
  - migration/compat:
    - preserved field names/shape; existing blocks remain valid; future events can extend schema safely

4. wrote a paper! 
Title: vn-IPFS (paper_restyled) — Plain-English Summary and Symbol Guide

Purpose
This note explains the core ideas, results, and evidence from paper_restyled.tex in plain, non-technical language. It also includes a glossary for the math symbols used, so you can justify the arguments to non-math audiences.

Big Picture (What this system does)
- Goal: reliable file replication in decentralized networks without DHTs (global routing tables) or central coordinators.
- Coordination (who should store a copy) happens via a shared "tuple space" (think: a shared bulletin board of small facts) spread over a lightweight ring overlay. Actual bytes move over libp2p streams.
- A simple placement rule (N-hop exclusion) pushes replicas away from the uploader’s neighborhood so copies end up meaningfully far apart.
- The ring layer (UVR) keeps only neighbor pointers (predecessor/successor) and fixes broken links locally when nodes fail. No global tables.

Key Ideas by Section
1) Introduction
- Problem: Replication clumps near the uploader or depends on global tables that are fragile under churn/partitions.
- Solution: Separate coordination from transport. Use a ring + tuple space for intent/state and libp2p streams for bytes. Enforce N-hop exclusion for outward diffusion.

2) Related Work (short context)
- DHTs optimize lookups but impose global invariants and repair costs. Gossip and CRDT-style (merge-without-conflict) techniques scale with local rules. This work favors local rules and convergence-friendly state over global routing.

3) Motivation vs. DHTs
- Why not DHTs: global tables to maintain; repair under churn; no direct control of replica placement.
- Our benefits: local invariants, bounded local repairs, explicit outward placement.

4) UVR Tuple-Space Semantics (how it is organized)
- UVR provides neighbor maintenance, best-effort spreading of facts, and local failure repair.
- Tuple space holds intent, progress, and compact summaries; idempotent merges prevent corruption from duplicates.

5) Replication Theory: N-Hop Exclusion (why copies go outward)
- Rule: when a node accepts a replica, it excludes itself and its neighbors from the next placements. Later acceptances therefore tend to be farther away from the origin.
- This creates an outward-moving frontier in expectation without needing full network paths.

6) Formal Proofs (what is guaranteed, explained simply)
6a) Convergence after things calm down (finite-height convergence)
- Claim: After a calm time T (no new facts), and with fair repeated delivery, every node’s state stabilizes to the same final state J.
- Premises: (P1) No new facts after T. (P2) Every update only adds information (idempotent, monotone merge). (P3) The space of facts is finite (finite-height). (P4) Within a connected component, any fact that is re-emitted indefinitely is delivered to everyone infinitely often (fairness).
- Goal: Show that for each node v, after some time T' ≥ T, x_v(t) = J for all t ≥ T'.
- Steps:
  1) Upper bound: Because new facts stop after T and merges only add, each x_v(t) can never exceed J (the union/merge of all facts present at T). So x_v(t) ⊑ J for all t ≥ T.
  2) Finite increases: Since facts come from a finite set and merges are idempotent, each node can only add a new fact finitely many times before it has seen everything in J.
  3) Eventual inclusion: Pick any single fact f that is present somewhere at time T. Because of connectedness and fairness, f (or messages carrying it) is seen infinitely often at every node. If v is missing f, the next time f arrives v merges it; by idempotence, duplicates change nothing.
  4) Exhaustion: Repeat Step 3 over all facts in J. There are finitely many, so after finitely many successful merges, every node contains all of J.
  5) Stabilization: Once x_v(t) = J, later merges are no-ops by idempotence. Hence each node stabilizes and all stabilized states are equal to J.
- Why valid: Each step uses only monotone growth, finiteness, and fairness. No retractions are needed, so order and delays don’t hurt correctness.

6b) Replication liveness (it finishes)
- Claim: Within a connected component with enough distinct eligible nodes, the process completes all n placements after T.
- Premises: (P1) After T the component is strongly connected with fair delivery. (P2) Re-emissions continue (no hard TTL before completion). (P3) Eligibility is greedy and idempotent: an eligible node accepts once, then becomes ineligible. (P4) Capacity: at least n distinct eligible nodes exist and remain live.
- Goal: Show that there is a finite time T' by which n distinct nodes have accepted and the remaining count reaches zero.
- Steps:
  1) Progress opportunity: If remaining n(t) > 0 and some node is eligible, fairness ensures that node will observe the tuple infinitely often.
  2) Greedy acceptance: On observing the tuple with n > 0, the eligible node accepts (policy), reducing n by exactly one.
  3) Idempotence: That node cannot accept again for the same content (already stored; now excluded), so each acceptance is unique and permanent.
  4) Bounded steps: At most the initial n acceptances are needed; after each acceptance, n decreases by one and never increases.
  5) Finite time: Re-emissions occur within bounded intervals, deliveries occur under fairness, so each acceptance occurs after a finite waiting time. Summing finitely many such waits yields finite completion time.
- Why valid: Monotone decrease of n and one-shot acceptance prevent oscillation; fairness prevents starvation; capacity ensures we don’t run out of distinct acceptors.

6c) Local repair time (bounded)
- Claim: When a successor fails, ring invariants are restored within a bounded time: tau_repair ≤ tau_fd + H + F.
- Premises: (P1) Heartbeats to the successor occur at most every H_hb units. (P2) Suspicion after m consecutive misses: tau_fd ≤ m*H_hb + F. (P3) Once suspicious, control messages are emitted within H and delivered within F (fairness window). (P4) Local successor recomputation preserves ring invariants and completes once messages are delivered.
- Goal: Bound time from crash to restored ring.
- Steps:
  1) Detection bound: After the crash, missed heartbeats trigger suspicion within tau_fd ≤ m*H_hb + F.
  2) Repair initiation: Upon suspicion, the predecessor emits repair/control tuples within H.
  3) Delivery: Those tuples reach relevant neighbors within an additional F.
  4) First consistent update: With those messages, the predecessor installs a new live successor, restoring invariants locally.
  5) Total bound: Summing yields tau_repair ≤ tau_fd + H + F.
- Why valid: Each phase is guarded by a local timer or fairness bound; idempotent updates ensure extra messages do not break invariants.

6d) Safety and duplicate suppression (no oscillation)
- Claim: Accepting the same content twice at a node has no effect, and the remaining count n never increases.
- Premises: (P1) Store is idempotent (writing an already stored content is a no-op). (P2) Acceptance adds the node to the exclusion set permanently. (P3) Only acceptances change n and each acceptance decrements n by one.
- Steps:
  1) At-most-once: After acceptance, the node stores the content and is excluded; subsequent deliveries don’t re-enable acceptance.
  2) Monotone n: Only acceptances affect n; duplicates are no-ops; thus n(t) is nonincreasing.
  3) Clean termination: When n reaches 0, eligibility is effectively off everywhere and the process halts.
- Why valid: Idempotence and monotone exclusion prevent double counting and oscillation.

6e) N-hop frontier growth (why placements move outward)
- Claim: The excluded region E_k grows monotonically and, when a new acceptance covers the next shell, the covered radius increases by one.
- Premises: (P1) After k acceptances A_k = {a_1, …, a_k}, E_k = B(o, h0) ∪ (union over i of B(a_i, h)). (P2) Acceptance at step k+1 occurs only outside E_k. (P3) The “covered radius” R_k is the largest r with B(o, r) ⊆ E_k.
- Goal: Show: (i) E_{k+1} ⊇ E_k and R_{k+1} ≥ R_k; (ii) if B(a_{k+1}, h) intersects the next shell S(o, R_k+1), then R_{k+1} ≥ R_k+1.
- Steps:
  1) Monotone frontier: E_{k+1} = E_k ∪ B(a_{k+1}, h) ⊇ E_k, so anything covered before remains covered. Hence R_{k+1} ≥ R_k.
  2) No re-entry: Since acceptance requires a_{k+1} ∉ E_k, it cannot be “inside” previously covered neighborhoods.
  3) Hitting the next shell: If B(a_{k+1}, h) overlaps S(o, R_k+1), then E_{k+1} contains all of B(o, R_k) plus some of the next shell; thus B(o, R_k+1) ⊆ E_{k+1}.
  4) Radius increase: Therefore R_{k+1} ≥ R_k+1.
- Why valid: Exclusions are unions of neighborhoods; unions only grow with new acceptances. The shell argument formalizes “outward drift” without needing shortest paths.

7) Experiments (evidence it works)
- Feasibility: Ring formation time grows roughly linearly with the number of nodes and has low variability. End-to-end replication shows steady latency with higher throughput for larger files.
- Adaptability: Microbenchmarks are consistent across hop counts with peaks at 16 and 512; throughput scales with offered load until resource limits.
- Robustness: With induced failures, neighbor-only repair recovers 100% with ~100 ms mean repair. Tuple spreading shows fast completion for most messages.

8) Significance and Tradeoffs
- Significance: Achieves outward diffusion without global routing tables, keeps repairs local, and relies on monotone (never backtracking) coordination.
- Tradeoffs: More coordination messages than direct point-to-point control, but much simpler failure handling and no global table maintenance.

9) Practical Implications (short operational view)
- Use tuple space + UVR for coordination and libp2p for bytes. Enforce N-hop exclusion at the app layer. Prefer explicit bootstraps over mDNS. Publish compact ring-info and replication-status. Sign and rate-limit control traffic.

Assumptions and Limits (in plain terms)
- Honest-but-fallible nodes: crashes and message drops can happen, but no lying (non-Byzantine).
- Eventual connectivity and fairness: after a calm period, repeated messages reach everyone in a connected component.
- No new facts during the calm period for the convergence guarantee.
- In partitions, each component progresses independently; when reconnected, time limits and versions prevent replay storms.

Glossary: Symbols and Terms (plain-English, ASCII names)
- G = (V, E): network graph. V = nodes (computers). E = links (who can talk to whom).
- R = (V, A): ring overlay. A = directed successor links (each node points to a successor).
- T = (cid, n, excluded, originator): replication task (content ID, remaining replicas needed, who is excluded, who started it).
- B(v, r): nodes within r hops from v (everyone up to r steps away).
- S(v, r): nodes exactly r hops from v (the ring at distance r).
- E_k: excluded set after k acceptances (places we will not place next).
- R_k: largest radius around the originator fully covered by exclusions after k acceptances (how far the no-go zone has grown).
- n: remaining replica count (counts down to 0).
- J = sqcup_u x_u(T): merged final state across a component at time T. "sqcup" is a join/merge that adds missing facts without duplicates.
- sqcup (join): merge two states by adding whatever one has that the other lacks, with deduplication.
- sqsubseteq (is contained in): X sqsubseteq Y means Y includes all info in X (and possibly more).
- cup (union): combine two sets into one containing all elements from both.
- subseteq (subset): X subseteq Y means every element of X is in Y.
- |X| (cardinality): the number of elements in set X.
- O(1): constant size or cost; does not grow with network size.
- tau_fd: failure detection time bound.
- tau_repair: repair completion time bound.
- H_hb: heartbeat interval to check successors.
- m: miss threshold for suspicion (consecutive missed heartbeats).
- F: fairness window (max delay to deliver messages that are retried indefinitely).
- H: retransmission bound (max interval between re-emissions).
- N (naturals): 0, 1, 2, ...
- TTL: time-to-live; a limit after which a message is discarded.
- Idempotent: doing the same update twice has the same effect as once.
- Monotone: state only grows by adding new facts; nothing is removed.
- Quiescent epoch: a calm period when no new facts are injected.
- Fairness: if a message is retried forever, everyone eventually sees it.
- CRDT: a data type that merges in any order without conflicts.
- UVR: Unidirectional Virtual Ring; each node keeps predecessor/successor, enabling simple repairs and predictable spreading.
- Tuple space: shared board of small facts; nodes put/get facts and the system spreads them.

How to explain the proofs to non-mathematicians
- Convergence: After things stop changing and people keep sharing, everyone ends up with the same full set of facts, and then nothing changes anymore.
- Liveness: If you need N placements and each placement reduces the remaining count by one, and nobody places twice, then after N placements you are done.
- Repair time: Because we check neighbors regularly and have a fallback, there is a bound on how long it takes to fix a broken link.

Takeaways
- Simple local rules (neighbor-only ring, outward placement, idempotent merges) can replace global routing tables for replication placement.
- Separating coordination (facts) from transport (bytes) makes failures easier to tolerate and reason about.
- The math formalizes common-sense guarantees: eventual agreement on facts, guaranteed completion given enough candidates, and bounded repair time.


5. 